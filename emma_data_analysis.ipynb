{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMMA Public Solicitations Data Analysis\n",
    "\n",
    "This notebook connects to the PostgreSQL database and analyzes the EMMA Maryland public solicitations data collected by our Dagster pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "from sqlalchemy import create_engine, text\n",
    "from datetime import datetime\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection setup\n",
    "DB_URL = \"postgresql://postgres:St0ck!adePG@localhost:5432/engineering\"\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "engine = create_engine(DB_URL)\n",
    "\n",
    "# Test connection\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(\"SELECT version();\"))\n",
    "    version = result.fetchone()[0]\n",
    "    print(f\"Connected to PostgreSQL: {version}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Get the raw HTML content for analysis\nfrom bs4 import BeautifulSoup\nimport re\n\nquery_html = \"\"\"\nSELECT \n    id,\n    timestamp,\n    value->>'raw_html' as raw_html\nFROM bronze.emma_public_solicitations\nORDER BY timestamp DESC\nLIMIT 1;\n\"\"\"\n\ndf_html = pd.read_sql(query_html, engine)\nraw_html = df_html.iloc[0]['raw_html'] if len(df_html) > 0 else None\n\nprint(f\"Retrieved HTML content of length: {len(raw_html) if raw_html else 0}\")\nprint(f\"Timestamp: {df_html.iloc[0]['timestamp'] if len(df_html) > 0 else 'None'}\")\n\n# Also show basic metadata\nquery_metadata = \"\"\"\nSELECT \n    id,\n    timestamp,\n    created_at,\n    length(value->>'raw_html') as html_length,\n    value->>'url' as url,\n    value->>'status' as status,\n    value->>'body_length' as body_length\nFROM bronze.emma_public_solicitations\nORDER BY timestamp DESC;\n\"\"\"\n\ndf_metadata = pd.read_sql(query_metadata, engine)\nprint(f\"Found {len(df_metadata)} records in the database\")\ndisplay(df_metadata)"
  },
  {
   "cell_type": "code",
   "source": "# Inspect table 11 specifically (the one with 26 rows)\nif raw_html:\n    soup = BeautifulSoup(raw_html, 'html.parser')\n    tables = soup.find_all('table')\n    \n    if len(tables) >= 11:\n        table_11 = tables[10]  # Table 11 (0-indexed)\n        rows = table_11.find_all('tr')\n        \n        print(f\"=== TABLE 11 DETAILED INSPECTION ===\")\n        print(f\"Total rows: {len(rows)}\")\n        \n        # Show header row\n        if len(rows) > 0:\n            header_cells = [cell.get_text(strip=True) for cell in rows[0].find_all(['th', 'td'])]\n            print(f\"Header row ({len(header_cells)} columns): {header_cells}\")\n        \n        # Show first few data rows\n        print(f\"\\nFirst 5 data rows:\")\n        for i, row in enumerate(rows[1:6], 1):  # Show rows 1-5\n            cells = [cell.get_text(strip=True) for cell in row.find_all(['td', 'th'])]\n            print(f\"Row {i} ({len(cells)} columns): {cells}\")\n        \n        # Check if any rows have links or special content\n        print(f\"\\nChecking for links in first few rows:\")\n        for i, row in enumerate(rows[1:4], 1):\n            links = row.find_all('a')\n            if links:\n                print(f\"Row {i} has {len(links)} links: {[a.get('href') for a in links]}\")\n            else:\n                print(f\"Row {i} has no links\")\n                \n        print(f\"\\nTable 11 structure looks good for extraction!\")\n    else:\n        print(f\"Cannot access table 11 - only {len(tables)} tables found\")\nelse:\n    print(\"No HTML content available\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Extract solicitation data from table 11 specifically (the correct table with 26 rows)\nsolicitations_data = []\n\nif raw_html:\n    soup = BeautifulSoup(raw_html, 'html.parser')\n    \n    # Get all tables\n    tables = soup.find_all('table')\n    print(f\"Found {len(tables)} total tables\")\n    \n    # Target table 11 (index 10) specifically - the one with actual solicitation data\n    if len(tables) >= 11:\n        target_table = tables[10]  # Table 11 (0-indexed)\n        rows = target_table.find_all('tr')\n        \n        print(f\"Table 11 has {len(rows)} rows (including header)\")\n        \n        if len(rows) > 1:  # Has header and data rows\n            # Get headers from first row\n            header_row = rows[0]\n            headers = [th.get_text(strip=True) for th in header_row.find_all(['th', 'td'])]\n            \n            print(f\"Headers: {headers}\")\n            \n            # Extract data rows (skip header row)\n            data_rows_processed = 0\n            for i, row in enumerate(rows[1:], 1):  # Skip header, start counting from 1\n                cells = [td.get_text(strip=True) for td in row.find_all(['td', 'th'])]\n                \n                # Only process rows that have actual data (not empty)\n                if len(cells) > 0 and any(cell.strip() for cell in cells):\n                    # Ensure we have the right number of columns\n                    while len(cells) < len(headers):\n                        cells.append('')\n                    \n                    # Create a record with column names\n                    record = {}\n                    for j, header in enumerate(headers):\n                        if j < len(cells):\n                            record[header] = cells[j]\n                        else:\n                            record[header] = ''\n                    \n                    # Add metadata\n                    record['row_number'] = i\n                    record['original_row_length'] = len(cells)\n                    \n                    solicitations_data.append(record)\n                    data_rows_processed += 1\n                    \n                    # Show first 3 rows for debugging\n                    if i <= 3:\n                        print(f\"Row {i}: {cells[:3]}...\")\n            \n            print(f\"Processed {data_rows_processed} data rows\")\n        else:\n            print(\"Table 11 has no data rows\")\n    else:\n        print(f\"Only found {len(tables)} tables, cannot access table 11\")\n\n# Create DataFrame\nif solicitations_data:\n    df_solicitations = pd.DataFrame(solicitations_data)\n    print(f\"\\nExtracted {len(df_solicitations)} solicitation records from table 11\")\n    print(f\"Columns: {list(df_solicitations.columns)}\")\n    \n    # Clean up the data\n    # Remove empty columns if any\n    df_solicitations = df_solicitations.dropna(how='all', axis=1)\n    \n    # Clean up column names (remove extra spaces, etc.)\n    df_solicitations.columns = [col.strip() for col in df_solicitations.columns]\n    \n    # Display basic info\n    print(f\"\\nDataFrame shape: {df_solicitations.shape}\")\n    print(f\"Cleaned columns: {list(df_solicitations.columns)}\")\n    \n    # Display first few records\n    print(\"\\nFirst 5 solicitations:\")\n    display(df_solicitations.head())\n    \n    # Show some statistics\n    if 'Status' in df_solicitations.columns:\n        print(f\"\\nStatus distribution:\")\n        print(df_solicitations['Status'].value_counts())\n        \n    if 'Due / Close Date' in df_solicitations.columns:\n        print(f\"\\nDue dates (first 10):\")\n        print(df_solicitations['Due / Close Date'].head(10))\n        \nelse:\n    print(\"No solicitation data extracted from table 11\")\n    df_solicitations = pd.DataFrame()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data analysis and insights\n",
    "if not df_solicitations.empty:\n",
    "    print(\"=== EMMA Solicitations Data Analysis ===\")\n",
    "    print(f\"Total solicitations: {len(df_solicitations)}\")\n",
    "    print(f\"Columns available: {len(df_solicitations.columns)}\")\n",
    "    \n",
    "    # Show data types and non-null counts\n",
    "    print(\"\\nData Info:\")\n",
    "    print(df_solicitations.info())\n",
    "    \n",
    "    # Show column names and sample data\n",
    "    print(\"\\nColumn Analysis:\")\n",
    "    for col in df_solicitations.columns:\n",
    "        unique_count = df_solicitations[col].nunique()\n",
    "        sample_value = df_solicitations[col].iloc[0] if len(df_solicitations) > 0 else 'N/A'\n",
    "        print(f\"  {col}: {unique_count} unique values, sample: '{str(sample_value)[:50]}...'\")\n",
    "        \n",
    "    # Look for date columns and parse them\n",
    "    date_columns = [col for col in df_solicitations.columns if 'date' in col.lower()]\n",
    "    if date_columns:\n",
    "        print(f\"\\nFound date columns: {date_columns}\")\n",
    "        \n",
    "    # Look for agency/department information\n",
    "    agency_columns = [col for col in df_solicitations.columns if any(kw in col.lower() for kw in ['agency', 'department', 'entity'])]\n",
    "    if agency_columns:\n",
    "        print(f\"\\nFound agency columns: {agency_columns}\")\n",
    "        for col in agency_columns:\n",
    "            print(f\"  {col} unique values: {df_solicitations[col].value_counts().head()}\")\n",
    "else:\n",
    "    print(\"No data to analyze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the extracted data to a CSV file for further analysis\n",
    "if not df_solicitations.empty:\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    csv_filename = f\"emma_solicitations_{timestamp}.csv\"\n",
    "    \n",
    "    df_solicitations.to_csv(csv_filename, index=False)\n",
    "    print(f\"Saved {len(df_solicitations)} records to {csv_filename}\")\n",
    "    \n",
    "    # Also save metadata\n",
    "    metadata_filename = f\"emma_metadata_{timestamp}.csv\"\n",
    "    df_metadata.to_csv(metadata_filename, index=False)\n",
    "    print(f\"Saved metadata to {metadata_filename}\")\n",
    "else:\n",
    "    print(\"No data to save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final summary\n",
    "print(\"=== Summary ===\")\n",
    "print(f\"Database records: {len(df_metadata)}\")\n",
    "print(f\"Extracted solicitations: {len(df_solicitations) if 'df_solicitations' in locals() else 0}\")\n",
    "print(f\"Data collection timestamp: {df_metadata.iloc[0]['timestamp'] if len(df_metadata) > 0 else 'None'}\")\n",
    "print(f\"HTML content size: {df_metadata.iloc[0]['html_length'] if len(df_metadata) > 0 else 0} characters\")\n",
    "\n",
    "if 'df_solicitations' in locals() and not df_solicitations.empty:\n",
    "    print(f\"Available columns: {', '.join(df_solicitations.columns)}\")\n",
    "    \n",
    "print(\"\\nNotebook analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}